{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34531de9-6ac7-47d4-bada-905da30ee89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark ML Classification Demo: Iris\n",
    "\n",
    "This notebook demonstrates a complete end-to-end classification workflow in Spark ML using the classic Iris dataset. It includes:\n",
    "- Data download into DBFS\n",
    "- Preprocessing and model pipeline\n",
    "- Training and evaluation metrics (accuracy, F1, weighted precision/recall)\n",
    "- Confusion matrix\n",
    "- Model introspection (coefficients or feature importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8e8cea-df9d-4fb0-a3da-603af6f893cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import requests, os\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efcf7ca-8414-4fd1-bf73-d6ba832a83b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets for configuration\n",
    "dbutils.widgets.text(\"data_url\", \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\", \"Data URL (CSV)\")\n",
    "dbutils.widgets.text(\"dbfs_output_path\", \"dbfs:/tmp/datasets/iris.csv\", \"DBFS output path\")\n",
    "dbutils.widgets.dropdown(\"classifier\", \"logistic_regression\", [\"logistic_regression\",\"random_forest\",\"gbt\"], \"Classifier\")\n",
    "dbutils.widgets.text(\"test_size\", \"0.2\", \"Test fraction (0-1)\")\n",
    "dbutils.widgets.text(\"random_seed\", \"42\", \"Random seed\")\n",
    "dbutils.widgets.dropdown(\"standardize_features\", \"true\", [\"true\",\"false\"], \"Standardize numeric features\")\n",
    "dbutils.widgets.dropdown(\"force_download\", \"false\", [\"false\",\"true\"], \"Force re-download data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5719a3d8-29db-444b-a2e4-216836830233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data URL: https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\nDBFS path: dbfs:/tmp/datasets/iris.csv\nClassifier: logistic_regression, standardize: True\nTest fraction: 0.2, seed: 42\nForce download: False\n"
     ]
    }
   ],
   "source": [
    "# Read widget values\n",
    "data_url = dbutils.widgets.get(\"data_url\").strip()\n",
    "dbfs_output_path = dbutils.widgets.get(\"dbfs_output_path\").strip()\n",
    "clf_choice = dbutils.widgets.get(\"classifier\").strip()\n",
    "test_fraction = float(dbutils.widgets.get(\"test_size\"))\n",
    "seed = int(float(dbutils.widgets.get(\"random_seed\")))\n",
    "standardize = dbutils.widgets.get(\"standardize_features\") == \"true\"\n",
    "force_download = dbutils.widgets.get(\"force_download\") == \"true\"\n",
    "\n",
    "print(f\"Data URL: {data_url}\")\n",
    "print(f\"DBFS path: {dbfs_output_path}\")\n",
    "print(f\"Classifier: {clf_choice}, standardize: {standardize}\")\n",
    "print(f\"Test fraction: {test_fraction}, seed: {seed}\")\n",
    "print(f\"Force download: {force_download}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5923dbe1-2c24-4d51-b394-de1de590c521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utilities to download and cache data in DBFS\n",
    "def dbfs_to_local(dbfs_path: str) -> str:\n",
    "    \"\"\"Convert dbfs:/path to /dbfs/path so Python can write the file.\"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:/\"):\n",
    "        return \"/dbfs/\" + dbfs_path.replace(\"dbfs:/\", \"\")\n",
    "    elif dbfs_path.startswith(\"/dbfs/\"):\n",
    "        return dbfs_path\n",
    "    else:\n",
    "        return \"/dbfs/\" + dbfs_path.lstrip(\"/\")\n",
    "\n",
    "def ensure_parent_dir(local_path: str):\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "def download_to_dbfs(url: str, dbfs_path: str, force: bool = False) -> str:\n",
    "    \"\"\"Stream-download a URL into DBFS, with simple caching.\"\"\"\n",
    "    local_path = dbfs_to_local(dbfs_path)\n",
    "    ensure_parent_dir(local_path)\n",
    "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0 and not force:\n",
    "        print(f\"File already exists at {dbfs_path} ({os.path.getsize(local_path)} bytes). Skipping download.\")\n",
    "        return dbfs_path\n",
    "    print(f\"Downloading {url} -> {dbfs_path}\")\n",
    "    with requests.get(url, stream=True, timeout=30) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp_path = local_path + \".tmp\"\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1<<14):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        os.replace(tmp_path, local_path)\n",
    "    print(\"Download complete.\")\n",
    "    return dbfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9116de72-1523-4b09-a167-88453df65391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv -> dbfs:/tmp/datasets/iris.csv\nDownload complete.\nSchema:\nroot\n |-- sepal_length: double (nullable = true)\n |-- sepal_width: double (nullable = true)\n |-- petal_length: double (nullable = true)\n |-- petal_width: double (nullable = true)\n |-- species: string (nullable = true)\n\nSample:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>species</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5.1,
         3.5,
         1.4,
         0.2,
         "setosa"
        ],
        [
         4.9,
         3.0,
         1.4,
         0.2,
         "setosa"
        ],
        [
         4.7,
         3.2,
         1.3,
         0.2,
         "setosa"
        ],
        [
         4.6,
         3.1,
         1.5,
         0.2,
         "setosa"
        ],
        [
         5.0,
         3.6,
         1.4,
         0.2,
         "setosa"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sepal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "species",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and load the dataset\n",
    "dbfs_csv = download_to_dbfs(data_url, dbfs_output_path, force_download)\n",
    "\n",
    "# Spark can read \"dbfs:/...\" paths directly\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(dbfs_csv)\n",
    "\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Sample:\")\n",
    "display(df.limit(5))\n",
    "\n",
    "# Basic validation for the Iris dataset\n",
    "expected_cols = {\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\",\"species\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV does not look like Iris dataset. Missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af17165e-4f3c-476c-9601-b0c7fe424c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>species</th><th>count</th></tr></thead><tbody><tr><td>virginica</td><td>50</td></tr><tr><td>versicolor</td><td>50</td></tr><tr><td>setosa</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "virginica",
         50
        ],
        [
         "versicolor",
         50
        ],
        [
         "setosa",
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "species",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA: label distribution\n",
    "display(df.groupBy(\"species\").count().orderBy(F.desc(\"count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c009654-8320-409b-a54f-572661a5f864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing and model selection\n",
    "label_col = \"species\"\n",
    "feature_cols = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "indexed_label_col = \"label\"\n",
    "features_vec_col = \"features\"\n",
    "scaled_features_col = \"scaledFeatures\" if standardize else features_vec_col\n",
    "\n",
    "# Transform label strings to indexed numeric labels\n",
    "label_indexer = StringIndexer(inputCol=label_col, outputCol=indexed_label_col, handleInvalid=\"error\")\n",
    "\n",
    "# Assemble numeric features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=features_vec_col, handleInvalid=\"error\")\n",
    "\n",
    "# Optionally standardize features (recommended for LR)\n",
    "scaler = StandardScaler(withMean=True, withStd=True, inputCol=features_vec_col, outputCol=scaled_features_col)\n",
    "\n",
    "# Choose classifier\n",
    "if clf_choice == \"logistic_regression\":\n",
    "    classifier = LogisticRegression(featuresCol=scaled_features_col, labelCol=indexed_label_col, maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
    "elif clf_choice == \"random_forest\":\n",
    "    classifier = RandomForestClassifier(featuresCol=scaled_features_col, labelCol=indexed_label_col, numTrees=100, maxDepth=5, seed=seed)\n",
    "elif clf_choice == \"gbt\":\n",
    "    # GBTClassifier is binary-only; fall back gracefully for a multiclass dataset\n",
    "    print(\"Warning: GBTClassifier supports binary classification only. Falling back to RandomForestClassifier.\")\n",
    "    classifier = RandomForestClassifier(featuresCol=scaled_features_col, labelCol=indexed_label_col, numTrees=200, maxDepth=6, seed=seed)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown classifier choice: {clf_choice}\")\n",
    "\n",
    "stages = [label_indexer, assembler] + ([scaler] if standardize else []) + [classifier]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d93b60d-2021-49c6-9d60-463a5e086fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 126, Test count: 24\n\uD83C\uDFC3 View run able-kit-423 at: https://adb-3017385027020604.4.azuredatabricks.net/ml/experiments/3784693748081254/runs/48495965e5814562a38ff974609a6ae1\n\uD83E\uDDEA View experiment at: https://adb-3017385027020604.4.azuredatabricks.net/ml/experiments/3784693748081254\n"
     ]
    }
   ],
   "source": [
    "# Train/test split and training\n",
    "train_df, test_df = df.randomSplit([1.0 - test_fraction, test_fraction], seed=seed)\n",
    "print(f\"Train count: {train_df.count()}, Test count: {test_df.count()}\")\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09d9238-a300-4064-a280-29dc7d2f125c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n{'accuracy': 0.9841269841269841, 'f1': 0.9841269841269841, 'weightedPrecision': 0.9841269841269841, 'weightedRecall': 0.9841269841269841}\nTest metrics:\n{'accuracy': 1.0, 'f1': 1.0, 'weightedPrecision': 1.0, 'weightedRecall': 1.0}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>pred_0</th><th>pred_1</th><th>pred_2</th></tr></thead><tbody><tr><td>6.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>7.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>11.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6.0,
         0.0,
         0.0
        ],
        [
         0.0,
         7.0,
         0.0
        ],
        [
         0.0,
         0.0,
         11.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "pred_0",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pred_1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pred_2",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label index mapping: {0: 'versicolor', 1: 'virginica', 2: 'setosa'}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation utilities\n",
    "def evaluate_predictions(pred_df, label_col=indexed_label_col, prediction_col=\"prediction\"):\n",
    "    \"\"\"\n",
    "    Compute common classification metrics and confusion matrix for a prediction DataFrame.\n",
    "    Returns a dict with metrics and confusion matrix (as nested lists).\n",
    "    \"\"\"\n",
    "    predictionAndLabels = pred_df.select(F.col(prediction_col).cast(\"double\"), F.col(label_col).cast(\"double\")).rdd.map(tuple)\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    labels = sorted([float(l) for l in pred_df.select(label_col).distinct().orderBy(label_col).rdd.map(lambda r: r[0]).collect()])\n",
    "    cm = metrics.confusionMatrix().toArray().tolist()\n",
    "    eval_acc = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"accuracy\").evaluate(pred_df)\n",
    "    eval_f1 = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"f1\").evaluate(pred_df)\n",
    "    eval_wp = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedPrecision\").evaluate(pred_df)\n",
    "    eval_wr = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col, metricName=\"weightedRecall\").evaluate(pred_df)\n",
    "    return {\n",
    "        \"labels\": labels,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"accuracy\": eval_acc,\n",
    "        \"f1\": eval_f1,\n",
    "        \"weightedPrecision\": eval_wp,\n",
    "        \"weightedRecall\": eval_wr,\n",
    "        \"precisionByLabel\": [metrics.precision(l) for l in labels],\n",
    "        \"recallByLabel\": [metrics.recall(l) for l in labels],\n",
    "        \"fMeasureByLabel\": [metrics.fMeasure(l) for l in labels],\n",
    "    }\n",
    "\n",
    "# Generate predictions\n",
    "train_pred = model.transform(train_df)\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "# Compute metrics\n",
    "train_stats = evaluate_predictions(train_pred)\n",
    "test_stats = evaluate_predictions(test_pred)\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print({k: v for k,v in train_stats.items() if k not in [\"confusion_matrix\",\"labels\",\"precisionByLabel\",\"recallByLabel\",\"fMeasureByLabel\"]})\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "print({k: v for k,v in test_stats.items() if k not in [\"confusion_matrix\",\"labels\",\"precisionByLabel\",\"recallByLabel\",\"fMeasureByLabel\"]})\n",
    "\n",
    "# Display confusion matrix for the test set\n",
    "import pandas as pd\n",
    "test_cm_df = pd.DataFrame(test_stats[\"confusion_matrix\"], index=[f\"true_{int(l)}\" for l in test_stats[\"labels\"]], columns=[f\"pred_{int(l)}\" for l in test_stats[\"labels\"]])\n",
    "display(test_cm_df)\n",
    "\n",
    "# Recover original species labels (index -> string)\n",
    "si_model = model.stages[0]  # StringIndexerModel\n",
    "id_to_label = {i: lbl for i, lbl in enumerate(si_model.labels)}\n",
    "print(\"Label index mapping:\", id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1f291e-7d7d-4057-a5c9-bc7187572341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression coefficients (multinomial):\nClass 0 (versicolor) intercept=11.1964 weights={'sepal_length': 4.102409055953124, 'sepal_width': -7.003273159281689, 'petal_length': 3.6580246040108375, 'petal_width': 4.461056427552989}\nClass 1 (virginica) intercept=-5.8866 weights={'sepal_length': 2.2766833809266664, 'sepal_width': -9.986849094451065, 'petal_length': 18.916705498969627, 'petal_width': 16.870575796944898}\nClass 2 (setosa) intercept=-5.3097 weights={'sepal_length': -6.37909243687979, 'sepal_width': 16.990122253732753, 'petal_length': -22.574730102980464, 'petal_width': -21.331632224497888}\n"
     ]
    }
   ],
   "source": [
    "# Model introspection: coefficients or feature importances\n",
    "from pyspark.ml.classification import LogisticRegressionModel, RandomForestClassificationModel, GBTClassificationModel\n",
    "\n",
    "last_stage = model.stages[-1]\n",
    "if isinstance(last_stage, LogisticRegressionModel):\n",
    "    print(\"Logistic Regression coefficients (multinomial):\")\n",
    "    coef = last_stage.coefficientMatrix.toArray()\n",
    "    intercepts = last_stage.interceptVector.toArray()\n",
    "    feature_names = feature_cols\n",
    "    import numpy as np  # noqa: F401\n",
    "    for cls_idx in range(coef.shape[0]):\n",
    "        cls_label = id_to_label.get(cls_idx, str(cls_idx))\n",
    "        weights = {feature_names[i]: float(coef[cls_idx, i]) for i in range(coef.shape[1])}\n",
    "        print(f\"Class {cls_idx} ({cls_label}) intercept={float(intercepts[cls_idx]):.4f} weights={weights}\")\n",
    "elif isinstance(last_stage, (RandomForestClassificationModel, GBTClassificationModel)):\n",
    "    fi = last_stage.featureImportances\n",
    "    feature_names = feature_cols\n",
    "    pairs = sorted(zip(feature_names, fi.toArray().tolist()), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Feature importances:\")\n",
    "    for name, importance in pairs:\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "else:\n",
    "    print(f\"Model type {type(last_stage)} not recognized for introspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f859f79-6a23-48cb-92f7-7e93101abdf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to run this notebook in Databricks\n",
    "\n",
    "1. Import this file as \"Source\":\n",
    "   - Workspace sidebar → right-click your folder → Import → Choose this .py file → Import format = Source.\n",
    "2. Attach to a cluster:\n",
    "   - Runtime: Apache Spark 3.x (DBR 11+ recommended).\n",
    "   - Python 3.8+ with MLlib included (standard runtimes suffice).\n",
    "3. Configure widgets at the top:\n",
    "   - data_url: public Iris CSV (default provided).\n",
    "   - dbfs_output_path: e.g., dbfs:/tmp/datasets/iris.csv\n",
    "   - classifier: logistic_regression, random_forest, or gbt (gbt falls back to RF).\n",
    "   - standardize_features: true for LR; either for trees.\n",
    "   - test_size and random_seed: control the split.\n",
    "   - force_download: true to re-fetch the CSV.\n",
    "4. Run cells from top to bottom. The notebook prints training/testing metrics and displays a confusion matrix.\n",
    "\n",
    "Troubleshooting:\n",
    "- Network/SSL issues when downloading? Set force_download=true or upload the CSV via UI (Data → DBFS → Upload) and point dbfs_output_path to it.\n",
    "- If you import as a standard notebook instead of source, the cells and markdown render normally; \"COMMAND ----------\" markers preserve cell boundaries in source files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf3453c-d6cc-49c0-a9d7-449b9dc45880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verification checklist\n",
    "\n",
    "Use this quick checklist after running the notebook top-to-bottom to verify everything is working:\n",
    "\n",
    "- Widgets appear at the top with defaults populated (data_url, dbfs_output_path, classifier, etc.).\n",
    "- Download step reports either \"Skipping download\" (file cached) or \"Download complete.\"\n",
    "- Schema prints with 5 columns: sepal_length, sepal_width, petal_length, petal_width, species; all numeric except species (string).\n",
    "- Class distribution display shows 3 classes (Iris-setosa, Iris-versicolor, Iris-virginica).\n",
    "- Train/Test counts are printed; both counts are &gt; 0.\n",
    "- Training and Test metrics are printed dictionaries including accuracy, f1, weightedPrecision, weightedRecall.\n",
    "- Test accuracy is typically &gt; 0.90 for Iris with LR or RF.\n",
    "- A confusion matrix table is displayed for the test set.\n",
    "- \"Label index mapping\" is printed, mapping 0/1/2 to species names.\n",
    "- Model introspection prints:\n",
    "  - Logistic Regression: intercepts and per-feature weights per class, or\n",
    "  - RandomForest: feature importances (descending).\n",
    "\n",
    "Optional next steps:\n",
    "- Replace the dataset URL with your own CSV that has a string label column and numeric features.\n",
    "- Add hyperparameter tuning via CrossValidator or TrainValidationSplit.\n",
    "- Log parameters/metrics/model to MLflow (Databricks: Experiment sidebar)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_ml_classification_demo",
   "widgets": {
    "classifier": {
     "currentValue": "logistic_regression",
     "nuid": "6b8b74a1-c8eb-4098-8a63-e15012635d57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "logistic_regression",
      "label": "Classifier",
      "name": "classifier",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "logistic_regression",
        "random_forest",
        "gbt"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "logistic_regression",
      "label": "Classifier",
      "name": "classifier",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "logistic_regression",
        "random_forest",
        "gbt"
       ]
      }
     }
    },
    "data_url": {
     "currentValue": "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv",
     "nuid": "577a48b9-5296-4664-bd05-5ef60ffe9786",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv",
      "label": "Data URL (CSV)",
      "name": "data_url",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv",
      "label": "Data URL (CSV)",
      "name": "data_url",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "dbfs_output_path": {
     "currentValue": "dbfs:/tmp/datasets/iris.csv",
     "nuid": "72adb582-e1d4-44df-a9d3-b91b288e444c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/tmp/datasets/iris.csv",
      "label": "DBFS output path",
      "name": "dbfs_output_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/tmp/datasets/iris.csv",
      "label": "DBFS output path",
      "name": "dbfs_output_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "force_download": {
     "currentValue": "false",
     "nuid": "79c2e781-0640-497a-8b19-2e8ed1f6226e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Force re-download data",
      "name": "force_download",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "false",
        "true"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Force re-download data",
      "name": "force_download",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "false",
        "true"
       ]
      }
     }
    },
    "random_seed": {
     "currentValue": "42",
     "nuid": "7d49d15b-0834-4632-ad2d-de8c80d8bac0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "42",
      "label": "Random seed",
      "name": "random_seed",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "42",
      "label": "Random seed",
      "name": "random_seed",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "standardize_features": {
     "currentValue": "true",
     "nuid": "21fb60b1-c214-412f-b4d4-44eb5e09b226",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": "Standardize numeric features",
      "name": "standardize_features",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "true",
      "label": "Standardize numeric features",
      "name": "standardize_features",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "test_size": {
     "currentValue": "0.2",
     "nuid": "34761981-632f-47b2-af12-30321a2bc563",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.2",
      "label": "Test fraction (0-1)",
      "name": "test_size",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.2",
      "label": "Test fraction (0-1)",
      "name": "test_size",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}